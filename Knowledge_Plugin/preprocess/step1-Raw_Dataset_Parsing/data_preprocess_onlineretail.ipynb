{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4ee02b49c0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copyright (c) Microsoft Corporation.\n",
    "# Licensed under the MIT license.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import gzip\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_pickle(data, filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def ReadLineFromFile(path):\n",
    "    lines = []\n",
    "    with open(path,'r') as fd:\n",
    "        for line in fd:\n",
    "            lines.append(line.rstrip('\\n'))\n",
    "    return lines\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "        \n",
    "'''\n",
    "Set seeds\n",
    "'''\n",
    "seed = 2022\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_data_name = \"online_retail\" # 'beauty' # \"sports\" # \"clothing\"\n",
    "full_data_name = \"online_retail\" # 'Beauty' # \"Sports_and_Outdoors\" # \"Clothing_Shoes_and_Jewelry\"\n",
    "if not os.path.exists(os.path.join(\"../../data/\", short_data_name)):\n",
    "    os.mkdir(os.path.join(\"../../data/\", short_data_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return (user, item, timestamp) sort in get_interaction\n",
    "def OnlineRetail(dataset_name, rating_score=3):\n",
    "    datas = []\n",
    "    data_dict = {}\n",
    "    data_file = \"../../data/raw_data/online_retail/data.csv\"\n",
    "    \n",
    "    with open(data_file, \"r\", encoding = 'unicode_escape') as fr:\n",
    "        fr.readline()\n",
    "        for line in tqdm(csv.reader(fr), desc=\"load all interactions\"):\n",
    "            InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, Country = line\n",
    "            if Description == '':\n",
    "                continue\n",
    "            user = InvoiceNo\n",
    "            item = StockCode\n",
    "            if (user, item) in data_dict:\n",
    "                continue\n",
    "            time = datetime.datetime.strptime(InvoiceDate, '%m/%d/%Y %H:%M').timestamp()\n",
    "            data_dict[(user, item)] = int(time) # merge duplicate interactions, keep the first record\n",
    "            datas.append((user, item, int(time)))\n",
    "    return datas\n",
    "\n",
    "def OnlineRetail_meta(dataset_name, datamaps):\n",
    "    meta_datas = {}\n",
    "    data_file = \"../../data/raw_data/online_retail/data.csv\"\n",
    "    item_ids = list(datamaps['item2id'].keys())\n",
    "\n",
    "    with open(data_file, \"r\", encoding = 'unicode_escape') as fr:\n",
    "        fr.readline()\n",
    "        for line in tqdm(csv.reader(fr), desc=\"load all interactions\"):\n",
    "            InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, Country = line\n",
    "            if Description == '' or StockCode not in item_ids:\n",
    "                continue\n",
    "            mapped_id = datamaps['item2id'][StockCode]\n",
    "            meta_datas[mapped_id] = {'item_id': StockCode, 'title': Description}\n",
    "    return meta_datas\n",
    "        \n",
    "def add_comma(num): # 1000000 -> 1,000,000\n",
    "    str_num = str(num)\n",
    "    res_num = ''\n",
    "    for i in range(len(str_num)):\n",
    "        res_num += str_num[i]\n",
    "        if (len(str_num)-i-1) % 3 == 0:\n",
    "            res_num += ','\n",
    "    return res_num[:-1]\n",
    "\n",
    "# get user interaction sequence for sequential recommendation\n",
    "def get_interaction(datas):\n",
    "    user_seq = {}\n",
    "    for data in datas:\n",
    "        user, item, time = data\n",
    "        if user in user_seq:\n",
    "            user_seq[user].append((item, time))\n",
    "        else:\n",
    "            user_seq[user] = []\n",
    "            user_seq[user].append((item, time))\n",
    "\n",
    "    for user, item_time in user_seq.items():\n",
    "        item_time.sort(key=lambda x: x[1])   \n",
    "        items = []\n",
    "        for t in item_time:\n",
    "            items.append(t[0])\n",
    "        user_seq[user] = items\n",
    "    return user_seq\n",
    "\n",
    "# K-core user_core item_core, return False if any user/item < core\n",
    "def check_Kcore(user_items, user_core, item_core):\n",
    "    user_count = defaultdict(int)\n",
    "    item_count = defaultdict(int)\n",
    "    for user, items in user_items.items():\n",
    "        for item in items:\n",
    "            user_count[user] += 1\n",
    "            item_count[item] += 1\n",
    "\n",
    "    for user, num in user_count.items():\n",
    "        if num < user_core:\n",
    "            return user_count, item_count, False\n",
    "    for item, num in item_count.items():\n",
    "        if num < item_core:\n",
    "            return user_count, item_count, False\n",
    "    return user_count, item_count, True  \n",
    "\n",
    "#  \n",
    "def filter_Kcore(user_items, user_core, item_core): #  \n",
    "    user_count, item_count, isKcore = check_Kcore(user_items, user_core, item_core)\n",
    "    while not isKcore:\n",
    "        for user, num in user_count.items():\n",
    "            if user_count[user] < user_core: #  \n",
    "                user_items.pop(user)\n",
    "            else:\n",
    "                for item in user_items[user]:\n",
    "                    if item_count[item] < item_core:\n",
    "                        user_items[user].remove(item)\n",
    "        user_count, item_count, isKcore = check_Kcore(user_items, user_core, item_core)\n",
    "    return user_items\n",
    "\n",
    "def id_map(user_items): # user_items dict\n",
    "    user2id = {} # raw 2 uid\n",
    "    item2id = {} # raw 2 iid\n",
    "    id2user = {} # uid 2 raw\n",
    "    id2item = {} # iid 2 raw\n",
    "    user_id = 1  # start from 1\n",
    "    item_id = 1\n",
    "    final_data = {}\n",
    "    random_user_list = list(user_items.keys())\n",
    "    random.shuffle(random_user_list)  # \n",
    "    for user in random_user_list:\n",
    "        items = user_items[user]\n",
    "        if user not in user2id:\n",
    "            user2id[user] = str(user_id)\n",
    "            id2user[str(user_id)] = user\n",
    "            user_id += 1\n",
    "        iids = [] # item id lists\n",
    "        for item in items:\n",
    "            if item not in item2id:\n",
    "                item2id[item] = str(item_id)\n",
    "                id2item[str(item_id)] = item\n",
    "                item_id += 1\n",
    "            iids.append(item2id[item])\n",
    "        uid = user2id[user]\n",
    "        final_data[uid] = iids\n",
    "    data_maps = {\n",
    "        'user2id': user2id,\n",
    "        'item2id': item2id,\n",
    "        'id2user': id2user,\n",
    "        'id2item': id2item\n",
    "    }\n",
    "    return final_data, user_id-1, item_id-1, data_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_name, acronym, data_type='OnlineRetail'):\n",
    "    assert data_type in {'Amazon', 'Yelp', 'Steam', 'OnlineRetail'}\n",
    "    rating_score = 0.0  # rating score smaller than this score would be deleted\n",
    "    # user 5-core item 5-core\n",
    "    user_core = 5\n",
    "    item_core = 5\n",
    "    attribute_core = 0\n",
    "\n",
    "    datas = OnlineRetail(data_name, rating_score)  # list of [user, item, timestamp]\n",
    "\n",
    "    user_items = get_interaction(datas) # dict of {user: interaction list sorted by time} \n",
    "    print(f'{data_name} Raw data has been processed! Lower than {rating_score} are deleted!')\n",
    "    # raw_id user: [item1, item2, item3...]\n",
    "    user_items = filter_Kcore(user_items, user_core=user_core, item_core=item_core)\n",
    "    print(f'User {user_core}-core complete! Item {item_core}-core complete!')\n",
    "\n",
    "    user_items, user_num, item_num, datamaps = id_map(user_items) # get mapping dicts, randomly shuffle\n",
    "    user_count, item_count, _ = check_Kcore(user_items, user_core=user_core, item_core=item_core)\n",
    "    user_count_list = list(user_count.values()) # user click count\n",
    "    user_avg, user_min, user_max = np.mean(user_count_list), np.min(user_count_list), np.max(user_count_list)\n",
    "    item_count_list = list(item_count.values()) # item click count\n",
    "    item_avg, item_min, item_max = np.mean(item_count_list), np.min(item_count_list), np.max(item_count_list)\n",
    "    interact_num = np.sum([x for x in user_count_list])\n",
    "    sparsity = (1 - interact_num / (user_num * item_num)) * 100\n",
    "    show_info = f'Total User: {user_num}, Avg User: {user_avg:.4f}, Min Len: {user_min}, Max Len: {user_max}\\n' + \\\n",
    "                f'Total Item: {item_num}, Avg Item: {item_avg:.4f}, Min Inter: {item_min}, Max Inter: {item_max}\\n' + \\\n",
    "                f'Iteraction Num: {interact_num}, Sparsity: {sparsity:.2f}%'\n",
    "    print(show_info)\n",
    "\n",
    "\n",
    "    print('Begin extracting meta infos...')\n",
    "    \n",
    "    meta_infos = OnlineRetail_meta(data_name, datamaps)\n",
    "\n",
    "    print(f'{data_name} & {add_comma(user_num)} & {add_comma(item_num)} & {user_avg:.1f}'\n",
    "          f'& {item_avg:.1f} & {add_comma(interact_num)} & {sparsity:.2f}\\% \\\\')\n",
    "\n",
    "    # -------------- Save Data ---------------\n",
    "    data_file = '../../data/{}/'.format(acronym) + 'sequential_data.txt'\n",
    "    metadata_file = '../../data/{}/'.format(acronym) + 'metadata.json'\n",
    "    datamaps_file = '../../data/{}/'.format(acronym) + 'datamaps.json'\n",
    "\n",
    "    with open(data_file, 'w') as out:\n",
    "        for user, items in user_items.items():\n",
    "            out.write(user + ' ' + ' '.join(items) + '\\n')\n",
    "\n",
    "    item_keys = sorted(meta_infos.keys(), key=lambda x: int(x))\n",
    "    print(f\"item2id: {len(datamaps['item2id'])}, meta_infos: {len(meta_infos)}, item_keys: {item_keys[:100]}\")\n",
    "    with open(metadata_file, 'w') as out:\n",
    "        for key in item_keys:\n",
    "            out.write(json.dumps(meta_infos[key]) + '\\n')\n",
    "\n",
    "    json_str = json.dumps(datamaps)\n",
    "    with open(datamaps_file, 'w') as out:\n",
    "        out.write(json_str)\n",
    "\n",
    "    # -------------- Split Train/Valid/Test for Item Import & Tagging ---------------\n",
    "    all_items = [item for item in datamaps['item2id'].keys()]\n",
    "    random.shuffle(all_items)\n",
    "    train_split = int(len(all_items) * 0.8)\n",
    "    valid_split = int(len(all_items) * 0.1)\n",
    "    train_items = all_items[:train_split]\n",
    "    valid_items = all_items[train_split:train_split+valid_split]\n",
    "    test_items = all_items[train_split+valid_split:]\n",
    "    outputs = {'train': train_items, 'val': valid_items, 'test': test_items}\n",
    "    save_pickle(outputs, '../../data/{}/item_splits.pkl'.format(short_data_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load all interactions: 541909it [00:05, 96299.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "online_retail Raw data has been processed! Lower than 0.0 are deleted!\n",
      "User 5-core complete! Item 5-core complete!\n",
      "Total User: 16517, Avg User: 31.1587, Min Len: 5, Max Len: 1092\n",
      "Total Item: 3466, Avg Item: 148.4850, Min Inter: 5, Max Inter: 2132\n",
      "Iteraction Num: 514649, Sparsity: 99.10%\n",
      "Begin extracting meta infos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load all interactions: 541909it [00:08, 64810.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "online_retail & 16,517 & 3,466 & 31.2& 148.5 & 514,649 & 99.10\\% \\\n",
      "item2id: 3466, meta_infos: 3466, item_keys: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100']\n"
     ]
    }
   ],
   "source": [
    "main(full_data_name, short_data_name, data_type='OnlineRetail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_test_data(data_name, test_num=99, sample_type='random'):\n",
    "    \"\"\"\n",
    "    sample_type:\n",
    "        random:  sample `test_num` negative items randomly.\n",
    "        pop: sample `test_num` negative items according to item popularity.\n",
    "    \"\"\"\n",
    "\n",
    "    data_file = f'sequential_data.txt'\n",
    "    if sample_type == 'random':\n",
    "        test_file = f'negative_samples.txt'\n",
    "    elif sample_type == 'pop':\n",
    "        test_file = f'negative_samples_pop.txt'\n",
    "\n",
    "    item_count = defaultdict(int)\n",
    "    user_items = defaultdict()\n",
    "\n",
    "    lines = open('../../data/{}/'.format(data_name) + data_file).readlines()\n",
    "    for line in tqdm(lines):\n",
    "        user, items = line.strip().split(' ', 1)\n",
    "        items = items.split(' ')\n",
    "        items = [int(item) for item in items]\n",
    "        user_items[user] = items\n",
    "        for item in items:\n",
    "            item_count[item] += 1\n",
    "\n",
    "    all_item = list(item_count.keys())\n",
    "    count = list(item_count.values())\n",
    "    sum_value = np.sum([x for x in count])\n",
    "    probability = [value / sum_value for value in count]\n",
    "\n",
    "    user_neg_items = defaultdict()\n",
    "\n",
    "    for user, user_seq in tqdm(user_items.items()):\n",
    "        test_samples = []\n",
    "        while len(test_samples) < test_num:\n",
    "            if sample_type == 'random':\n",
    "                sample_ids = np.random.choice(all_item, test_num, replace=False)\n",
    "            else: # sample_type == 'pop':\n",
    "                sample_ids = np.random.choice(all_item, test_num, replace=False, p=probability)\n",
    "            sample_ids = [str(item) for item in sample_ids if item not in user_seq and item not in test_samples]\n",
    "            test_samples.extend(sample_ids)\n",
    "        test_samples = test_samples[:test_num]\n",
    "        user_neg_items[user] = test_samples\n",
    "\n",
    "    with open('../../data/{}/'.format(data_name) + test_file, 'w') as out:\n",
    "        for user, samples in user_neg_items.items():\n",
    "            out.write(user+' '+' '.join(samples)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16517/16517 [00:00<00:00, 81231.99it/s]\n",
      "100%|██████████| 16517/16517 [03:12<00:00, 85.69it/s] \n"
     ]
    }
   ],
   "source": [
    "sample_test_data(short_data_name, test_num=99, sample_type=\"random\")  #sample 99 negative testing samples for each user"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "183e66c1b4a3cebcc9a59d658c813a22bf955a401fdefcad593bde0024c573fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
